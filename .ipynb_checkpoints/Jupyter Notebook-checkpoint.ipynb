{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b418d15f",
   "metadata": {},
   "source": [
    "# TIMESERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fab111",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "This project focuses on the application of time series regression analysis to forecast sales for Corporation Favorita, a prominent grocery retailer based in Ecuador.\n",
    "\n",
    "The primary objective is to develop a robust model capable of accurately forecasting future sales by leveraging the extensive time series data of thousands of products sold across various Favorita locations. The resulting forecasts will provide valuable insights to the store's management, enabling them to formulate effective inventory and sales plans.\n",
    "\n",
    "Through this research, we will construct models based on historical analysis, establish scientific hypothesis using time-stamped historical data, and employ these models to observe patterns and guide strategic decision-making in the future. By delving into the data, our aim is to optimize operations and ultimately drive sales growth for Favorita Corporation, supporting the management team in extracting meaningful insights from their vast dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b03023",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "Null Hypothesis: Sales are not affected by promotion, oil prices and holidays.\n",
    "\n",
    "Alternate Hypothesis: Sales are affected by promotion, oil prices and holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f35455",
   "metadata": {},
   "source": [
    "# Analytical Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230fc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "\n",
    "# !pip install pyodbc\n",
    "# !pip install python-dotenv\n",
    "# !pip install sqlalchemy\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost\n",
    "# !pip install catboost\n",
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd3d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as ndates\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries to create connection string to SQL server\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Library for imputing missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Library for seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Library for checking stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Library for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Library for feature encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Libraries for calculating evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "\n",
    "# Library to make series stationary\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Library for working with operating system\n",
    "import os\n",
    "\n",
    "# Library to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "829015cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    "    legend=False,\n",
    ")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c76e14",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "### Accessing and loading the datasets\n",
    "\n",
    "The first dataset was collected from a SQL database by first passing a connection string using the pyodbc library. Afterwards a SQL query was used to obtain the dataset. This is as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b7ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable in the .env file into a dictionary\n",
    "\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the .env file\n",
    "server = environment_variables.get('SERVER')\n",
    "database = environment_variables.get('DATABASE')\n",
    "username = environment_variables.get('USERNAME')\n",
    "password = environment_variables.get('PASSWORD')\n",
    "\n",
    "# The connection string is an f string that includes all the variable above to establish a connection to the server.\n",
    "connection_string = f'DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a79cebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the connect method of the pyodbc library to pass in the connection string.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Check your internet connection if it takes more time than necessary.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpyodbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get the oil dataset using the SQL query shown below\u001b[39;00m\n\u001b[0;32m      7\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelect * from dbo.oil\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')"
     ]
    }
   ],
   "source": [
    "# Use the connect method of the pyodbc library to pass in the connection string.\n",
    "# Check your internet connection if it takes more time than necessary.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)\n",
    "\n",
    "# Get the oil dataset using the SQL query shown below\n",
    "query1 = 'Select * from dbo.oil'\n",
    "oil = pd.read_sql(query1, connection)\n",
    "\n",
    "# Get the holiday dataset using the SQL query shown below\n",
    "query2 = 'Select * from dbo.holidays_events'\n",
    "holiday = pd.read_sql(query2, connection)\n",
    "\n",
    "# Get the stores dataset using the SQL query shown below\n",
    "query3 = 'Select * from dbo.stores'\n",
    "stores = pd.read_sql(query3, connection)\n",
    "\n",
    "# Save the datasets\n",
    "oil.to_csv(r'oil.csv')\n",
    "holiday.to_csv(r'holiday.csv')\n",
    "stores.to_csv(r'stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdded6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57598695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the other datasets\n",
    "\n",
    "transactions = pd.read_csv('transactions.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d681dc",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ba797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the holiday dataset\n",
    "\n",
    "holiday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the oil dataset\n",
    "\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c00cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices\n",
    "\n",
    "oil_df = oil.set_index('date')\n",
    "oil_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the stores dataset\n",
    "\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the transactions dataset\n",
    "\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the train dataset\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af332e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the test dataset\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the first five rows of the sample_submission dataset\n",
    "\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d622b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the datasets\n",
    "\n",
    "data = {'holiday': holiday, 'oil': oil, 'stores': stores, 'transactions': transactions, 'train': train, 'test': test, 'sample_submission': sample_submission}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d386d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the datatypes and presence of missing values in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m:')\n",
    "    dataset.info()\n",
    "    print('_'*45)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape, and the presence of missing values and duplicates in each of the datasets\n",
    "# Use '\\033[1mtext\\033[0m' to make text bold\n",
    "\n",
    "for df, dataset in data.items():\n",
    "    print(f'\\033[1mFor {df} dataset\\033[0m')\n",
    "    print(f'Shape: {dataset.shape}')\n",
    "    print(f'Missing values = {dataset.isna().sum().sum()}')\n",
    "    print(f'Duplicates = {dataset.duplicated().sum()}')\n",
    "    print('_'*30)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cf4ba",
   "metadata": {},
   "source": [
    "# Problems Identified\n",
    "\n",
    "The datasets are seperate, and need to be merged together for better analysis.\n",
    "\n",
    "The oil dataset has 43 missing values on the 'dcoilwtico' column which should be filled.\n",
    "\n",
    "Each of the 'date' columns have an object datatype instead of a datetime datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a92d6",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The problems identified with the datasets will be handled to prepare the data for analysis and modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b4579",
   "metadata": {},
   "source": [
    "### Merge the datasets based on common columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c11a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transactions dataset to train on 'date' and 'store_nbr' columns\n",
    "df1 = pd.merge(train, transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Merge holiday dataset to df1 on 'date' column\n",
    "df2 = pd.merge(df1, holiday, on='date', how='left')\n",
    "\n",
    "# Merge oil dataset to df2 on 'date' column\n",
    "df3 = pd.merge(df2, oil, on='date', how='left')\n",
    "df3\n",
    "\n",
    "# Merge store dataset to df3 on 'store_nbr' column\n",
    "df4 = pd.merge(df3, stores, on='store_nbr', how='left')\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates after merging the datasets\n",
    "\n",
    "df4.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0376f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset\n",
    "\n",
    "df = df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_x' column on df4\n",
    "\n",
    "df['type_x'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3141032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating 'type_y' column on df4\n",
    "\n",
    "df['type_y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46e3d8",
   "metadata": {},
   "source": [
    "As can be seen in the merged dataset, the column named type_x is the type column of the holiday dataset, the column named dcoilwtico represents the oil price in the oil dataset, while the column named type_y is the type column of the store dataset. These columns will be renamed for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'type_x', 'dcoilwtico' and type_y' to 'holiday_type', 'oil_price' and 'store_type' respectively\n",
    "\n",
    "df = df.rename(columns={'type_x': 'holiday_type', 'dcoilwtico': 'oil_price', 'type_y': 'store_type'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a133436",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab552692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after merging the datasets\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f4201",
   "metadata": {},
   "source": [
    "The missing values in the transactions column will be filled with 0 because it represents the absence of transactions on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbe438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the transactions column with 0\n",
    "\n",
    "df['transactions'].fillna(0, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4688fe9",
   "metadata": {},
   "source": [
    "For holiday_type, locale, locale_name, description and transferred columns, there are equal number of missing values. This is because these columns are from the holiday dataset, and they represent the days where there were no holidays. These empty cells will be filled with 'No holiday' for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in type_x, locale, locale_name, description and transferred columns with 'No holiday'\n",
    "columns_to_fill = ['holiday_type', 'locale', 'locale_name', 'description', 'transferred']\n",
    "for column in columns_to_fill:\n",
    "        df[column].fillna('No holiday', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values in these columns\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6938fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices before filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b693750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in the oil prices using backward fill to ensure continuity in the trend\n",
    "df['oil_price'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Confirm that there are no more missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05222fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trend of oil prices after filling the missing values\n",
    "\n",
    "df['oil_price'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e11af",
   "metadata": {},
   "source": [
    "### Change the datatype of the 'date' column from object to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of the date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dc1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from the 'date' column using pandas' powerful time-based indexing\n",
    "\n",
    "df['year'] = df.date.dt.year\n",
    "df['month'] = df.date.dt.month\n",
    "df['dayofmonth'] = df.date.dt.day\n",
    "df['dayofweek'] = df.date.dt.dayofweek\n",
    "df['dayname'] = df.date.dt.strftime('%A')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column as index\n",
    "\n",
    "df = df.set_index('date')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename df to train_merged\n",
    "\n",
    "train_merged = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 8 rows\n",
    "\n",
    "train_merged.sample(8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 'transactions' column\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = train_merged['transactions'].plot(linewidth=0.5)\n",
    "ax.set_ylabel('Transactions')\n",
    "ax.set_xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065742dc",
   "metadata": {},
   "source": [
    "The plot above reveals that transactions are always highest at the end of each year. This reveals seasonality in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition\n",
    "\n",
    "result = seasonal_decompose(train_merged['sales'], model='additive', period=365)\n",
    "result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for stationarity using adfuller\n",
    "\n",
    "# result = adfuller(series, autolag='AIC')\n",
    "\n",
    "# print(f'ADF Statistics: {result[0]}')\n",
    "# print(f'p-value: {result[1]}')\n",
    "# print(f'Critical Values: {result[4]}')\n",
    "\n",
    "# if result[1] > 0.05 :\n",
    "#     print('Series is not stationary')\n",
    "# else:\n",
    "#     print('Series is stationary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lag plot\n",
    "\n",
    "pd.plotting.lag_plot(train_merged['transactions'], lag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of the 'transactions' column grouped by 'locale'\n",
    "sns.boxplot(x='transactions', y='locale', data=train_merged)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c350c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the 'transactions' column\n",
    "train_merged.transactions.hist()\n",
    "\n",
    "# Add labels to the x-axis, y-axis, and title\n",
    "plt.xlabel('Transactions', fontsize=16)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.title('Histogram of Transactions', fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sales trend of the dataset\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x='date', y='sales', data=train_merged)\n",
    "plt.title('Sales Trend Of The Dataset')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333af892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics for numerical columns in train_data DataFrame\n",
    "\n",
    "train_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1429f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of train_data with numerical columns only\n",
    "train_merged_num = train_merged.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the correlation matrix of the numerical columns\n",
    "corr_matrix = train_merged_num.corr()\n",
    "\n",
    "# Visualizing the correlation matrix with a heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# Save the chart as an image file\n",
    "# plt.savefig('Correlation of the numerical columns of the train dataset.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2b16c",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7fdfa",
   "metadata": {},
   "source": [
    "# Answering Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee276df5",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n",
    "8. What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e26edb",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c5588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check the range of the date column\n",
    "dates_range = pd.date_range(start=train_merged.index.min(), end=train_merged.index.max())\n",
    "\n",
    "# Check for missing dates in the dataset\n",
    "missing_dates = set(dates_range.date) - set(train_merged.index.unique())\n",
    "\n",
    "# Create a new dataframe with the dates_missing data\n",
    "missing_dates_df = pd.DataFrame(missing_dates)\n",
    "missing_dates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f56e02",
   "metadata": {},
   "source": [
    "The dataset has some missing dates. This means that it is not complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842644a4",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0fafc",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time period before and after the earthquake\n",
    "pre_earthquake_start_date = '2016-04-01'\n",
    "pre_earthquake_end_date = '2016-04-15'\n",
    "post_earthquake_start_date = '2016-04-17'\n",
    "post_earthquake_end_date = '2016-04-30'\n",
    "\n",
    "# Filter the sales data before and after the earthquake\n",
    "pre_earthquake_sales = train_merged[(train_merged.index >= pre_earthquake_start_date) & (train_merged.index <= pre_earthquake_end_date)]\n",
    "post_earthquake_sales = train_merged[(train_merged.index >= post_earthquake_start_date) & (train_merged.index <= post_earthquake_end_date)]\n",
    "\n",
    "# Calculate the total sales before and after the earthquake\n",
    "pre_earthquake_total_sales = pre_earthquake_sales['sales'].sum()\n",
    "post_earthquake_total_sales = post_earthquake_sales['sales'].sum()\n",
    "\n",
    "# Visualize the sales data before and after the sales data\n",
    "labels = ['Pre-Earthquake', 'Post-Earthquake']\n",
    "total_sales = [pre_earthquake_total_sales, post_earthquake_total_sales]\n",
    "plt.bar(labels, total_sales)\n",
    "plt.xlabel('Before And After Earthquake')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Impact Of Earthquake On Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2a11f",
   "metadata": {},
   "source": [
    "The plot above shows that there was a slight increase in sales after the earthquake. This means that the earthquake did not affect the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d381900",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in the different store clusters\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='cluster', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Store Clusters')\n",
    "plt.xlabel('Store Clusters')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21819bb8",
   "metadata": {},
   "source": [
    "The plot above shows that stores in cluster 5 are making more sales than the stores in other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different cities\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='city', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different Cities')\n",
    "plt.xlabel('Cities')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb3b9",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Quito are making more sales than stores in other cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aababf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different states\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='state', y='sales', data=train_merged)\n",
    "plt.title('Sales In Different States')\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Sales')\n",
    "plt.xticks(rotation = 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077f5f3",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Pichincha are making more sales than stores in other states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3033312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales in different store types\n",
    "\n",
    "store_types = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', y='sales', data=train_merged, order=store_types)\n",
    "plt.title('Sales In Different Store Types')\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80024",
   "metadata": {},
   "source": [
    "The plot above shows that stores in Store type A are making more sales than stores in other store types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebbf59",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify families with at least one item on promotion and families with no item on promotion\n",
    "train_merged['onpromotion'] = train_merged['onpromotion'].apply(lambda x: 'No Promotion' if x == 0 else 'Promotion')\n",
    "\n",
    "# Group by promotion and sum the sales\n",
    "x = train_merged.groupby(['onpromotion'], as_index=False).agg({'sales':'sum'})\n",
    "\n",
    "# Plot the sales of promotion and non_promotion families)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x.onpromotion, x.sales)\n",
    "plt.title('Impact Of Promotion On Sales')\n",
    "plt.xticks((0,1))\n",
    "plt.xlabel('Promotion Status')\n",
    "plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624b7c3",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by promotion. Product families with items on promotion are being sold more than product families with no item on promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales with different oil prices\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x='oil_price', y='sales', data=train_merged)\n",
    "plt.title('Sales With Different Oil Prices')\n",
    "plt.xlabel('Oil Prices')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b8c34",
   "metadata": {},
   "source": [
    "The plot above shows that sales are affected by the oil prices. As shown, there are more number of sales and more volume of sales at lower oil prices than at higher oil prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the holiday events\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x='holiday_type', data=train_merged)\n",
    "plt.title('Count of Holiday Events By Type')\n",
    "plt.xlabel('Holiday Events')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba17dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sales on holidays with sales on non-holidays\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='holiday_type', y='sales', data=train_merged)\n",
    "plt.title('Sales On Holidays Vs Non-holidays')\n",
    "plt.xlabel('Is Holidays')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59394b4",
   "metadata": {},
   "source": [
    "There are more sales during holidays than when there are no holidays. This is because more people are free to go out for shopping during holidays than when there is no holiday and they have to be at work or school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bec01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare the sales on holidays with sales on non-holidays for the different store types\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='store_type', hue='holiday_type', y='sales', data=train_merged, ci=None, order=store_types)\n",
    "plt.title('Sales On Holidays Vs Non-holidays For Each Store Type')\n",
    "plt.xlabel('Is Holidays')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend(title='Holiday Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4779c",
   "metadata": {},
   "source": [
    "There is no signicant impact of holidays on the sales made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5ac0a",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68232d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From the date and its extractable features, the average sales on different days of the week can be analyzed\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='dayname', y='sales', data=train_merged, order=days)\n",
    "plt.title('Sales On Different Days Of The Week')\n",
    "plt.xlabel('Days Of The Week')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2442b5",
   "metadata": {},
   "source": [
    "From the date and its extractable features, we can observe the sales according to the days of the week. As shown above, there are more sales during the weekends, with peak sales on Sundays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ddbf1",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f05791",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What is the total sales made each year by the corporation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and sum of sales for each year\n",
    "\n",
    "sales_per_year = train_merged.groupby(['year'], as_index=False).agg({'sales':'sum'})\n",
    "sales_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee58d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sales made each year\n",
    "\n",
    "plt.bar(sales_per_year.year, sales_per_year.sales)\n",
    "plt.title('Sales Per Year',fontsize=14)\n",
    "plt.ylabel('Sales',fontsize=14)\n",
    "plt.xlabel('Year',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef1a67",
   "metadata": {},
   "source": [
    "From the data provided, Corporation Favorita made it's lowest sales in 2013 and its highest sales in 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54de21d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52463b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the train_merged dataset on which to perform feature engineering\n",
    "\n",
    "train_data = train_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cac279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index back to a normal column\n",
    "\n",
    "train_data = train_data.reset_index()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f0db9",
   "metadata": {},
   "source": [
    "### Drop Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the columns of the dataset\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random sample of 5 rows to see the contents of the columns\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns and display the dataset\n",
    "\n",
    "columns_to_drop = ['id', 'store_nbr', 'locale', 'locale_name', 'description', 'transferred', 'city', 'store_type', 'cluster', 'month', 'dayofmonth', 'dayofweek', 'dayname']\n",
    "\n",
    "train_data = train_data.drop(columns_to_drop, axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eddc7",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "The dataset will be splitted to training and validation sets using the time-based split. This will be done based on the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the years in the dataset\n",
    "\n",
    "train_data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the years for the training set and validation set\n",
    "train_years = [2013, 2014, 2015, 2016]\n",
    "val_year = [2017]\n",
    "\n",
    "# Obtain the training set and validation set\n",
    "train_set = train_data.loc[train_data['year'].isin(train_years)]\n",
    "val_set = train_data.loc[train_data['year'].isin(val_year)]\n",
    "\n",
    "# Print the shape of the training set and validation set\n",
    "train_set.shape, val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the datatypes of the columns of the training set\n",
    "\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the categorical columns to encode and numerical columns to scale\n",
    "cat_columns_to_encode = ['family', 'onpromotion', 'holiday_type', 'state']\n",
    "num_columns_to_scale = ['transactions', 'oil_price']\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of training set\n",
    "train_set_cat_df = train_set[cat_columns_to_encode]\n",
    "train_set_num_df = train_set[num_columns_to_scale]\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns of validation set\n",
    "val_set_cat_df = val_set[cat_columns_to_encode]\n",
    "val_set_num_df = val_set[num_columns_to_scale]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6877ec",
   "metadata": {},
   "source": [
    "### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549da07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder object using OneHotEncoder.\n",
    "# Set sparse=False for dense output and drop='first' to avoid multicollinearity\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the training set\n",
    "encoder.fit(train_set_cat_df)\n",
    "train_set_cat_encoded = encoder.transform(train_set_cat_df).tolist()\n",
    "train_set_cat_encoded_df = pd.DataFrame(train_set_cat_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# View the encoded columns of the training set\n",
    "train_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aed355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical columns of the training set\n",
    "encoder.fit(val_set_cat_df)\n",
    "val_set_cat_encoded = encoder.transform(val_set_cat_df).tolist()\n",
    "val_set_cat_encoded_df = pd.DataFrame(val_set_cat_encoded, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# View the encoded columns of the validation set\n",
    "val_set_cat_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb672032",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f480e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the training set\n",
    "scaler.fit(train_set_num_df)\n",
    "train_set_num_scaled = scaler.transform(train_set_num_df).tolist()\n",
    "train_set_num_scaled_df = pd.DataFrame(train_set_num_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled columns of the training set\n",
    "train_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed04e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScaler to scale the numerical columns of the validation set\n",
    "scaler.fit(val_set_num_df)\n",
    "val_set_num_scaled = scaler.transform(val_set_num_df).tolist()\n",
    "val_set_num_scaled_df = pd.DataFrame(val_set_num_scaled, columns=scaler.get_feature_names_out())\n",
    "\n",
    "# View the scaled columns of the validation set\n",
    "val_set_num_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the encoded and scaled DataFrames with the date and sales columns of train_set and val_set\n",
    "# to get the final training and validation sets\n",
    "\n",
    "train_final = pd.concat([train_set_cat_encoded_df, train_set_num_scaled_df, train_set[['date', 'sales']]], axis=1)\n",
    "val_final = pd.concat([val_set_cat_encoded_df, val_set_num_scaled_df, val_set[['date', 'sales']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e121b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc094e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2ad7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "191d3d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_BABY CARE</th>\n",
       "      <th>family_BEAUTY</th>\n",
       "      <th>family_BEVERAGES</th>\n",
       "      <th>family_BOOKS</th>\n",
       "      <th>family_BREAD/BAKERY</th>\n",
       "      <th>family_CELEBRATION</th>\n",
       "      <th>family_CLEANING</th>\n",
       "      <th>family_DAIRY</th>\n",
       "      <th>family_DELI</th>\n",
       "      <th>family_EGGS</th>\n",
       "      <th>...</th>\n",
       "      <th>family_SEAFOOD</th>\n",
       "      <th>onpromotion_Promotion</th>\n",
       "      <th>holiday_type_Bridge</th>\n",
       "      <th>holiday_type_Event</th>\n",
       "      <th>holiday_type_Holiday</th>\n",
       "      <th>holiday_type_No holiday</th>\n",
       "      <th>holiday_type_Transfer</th>\n",
       "      <th>holiday_type_Work Day</th>\n",
       "      <th>transactions</th>\n",
       "      <th>oil_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.474507</td>\n",
       "      <td>0.849157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.474507</td>\n",
       "      <td>0.849157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.474507</td>\n",
       "      <td>0.849157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.474507</td>\n",
       "      <td>0.849157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.474507</td>\n",
       "      <td>0.849157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   family_BABY CARE  family_BEAUTY  family_BEVERAGES  family_BOOKS   \n",
       "0               0.0            0.0               0.0           0.0  \\\n",
       "1               1.0            0.0               0.0           0.0   \n",
       "2               0.0            1.0               0.0           0.0   \n",
       "3               0.0            0.0               1.0           0.0   \n",
       "4               0.0            0.0               0.0           1.0   \n",
       "\n",
       "   family_BREAD/BAKERY  family_CELEBRATION  family_CLEANING  family_DAIRY   \n",
       "0                  0.0                 0.0              0.0           0.0  \\\n",
       "1                  0.0                 0.0              0.0           0.0   \n",
       "2                  0.0                 0.0              0.0           0.0   \n",
       "3                  0.0                 0.0              0.0           0.0   \n",
       "4                  0.0                 0.0              0.0           0.0   \n",
       "\n",
       "   family_DELI  family_EGGS  ...  family_SEAFOOD  onpromotion_Promotion   \n",
       "0          0.0          0.0  ...             0.0                    0.0  \\\n",
       "1          0.0          0.0  ...             0.0                    0.0   \n",
       "2          0.0          0.0  ...             0.0                    0.0   \n",
       "3          0.0          0.0  ...             0.0                    0.0   \n",
       "4          0.0          0.0  ...             0.0                    0.0   \n",
       "\n",
       "   holiday_type_Bridge  holiday_type_Event  holiday_type_Holiday   \n",
       "0                  0.0                 0.0                   1.0  \\\n",
       "1                  0.0                 0.0                   1.0   \n",
       "2                  0.0                 0.0                   1.0   \n",
       "3                  0.0                 0.0                   1.0   \n",
       "4                  0.0                 0.0                   1.0   \n",
       "\n",
       "   holiday_type_No holiday  holiday_type_Transfer  holiday_type_Work Day   \n",
       "0                      0.0                    0.0                    0.0  \\\n",
       "1                      0.0                    0.0                    0.0   \n",
       "2                      0.0                    0.0                    0.0   \n",
       "3                      0.0                    0.0                    0.0   \n",
       "4                      0.0                    0.0                    0.0   \n",
       "\n",
       "   transactions  oil_price  \n",
       "0     -1.474507   0.849157  \n",
       "1     -1.474507   0.849157  \n",
       "2     -1.474507   0.849157  \n",
       "3     -1.474507   0.849157  \n",
       "4     -1.474507   0.849157  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the encoded X_train DataFrame and the scaled X_train DataFrame to have the unbalanced X_train DataFrame\n",
    "\n",
    "X_train = pd.concat([X_train_cat_encoded_df, X_train_num_scaled_df.set_axis(X_train_cat_encoded_df.index)], axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5699dd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family_BABY CARE</th>\n",
       "      <th>family_BEAUTY</th>\n",
       "      <th>family_BEVERAGES</th>\n",
       "      <th>family_BOOKS</th>\n",
       "      <th>family_BREAD/BAKERY</th>\n",
       "      <th>family_CELEBRATION</th>\n",
       "      <th>family_CLEANING</th>\n",
       "      <th>family_DAIRY</th>\n",
       "      <th>family_DELI</th>\n",
       "      <th>family_EGGS</th>\n",
       "      <th>...</th>\n",
       "      <th>family_PRODUCE</th>\n",
       "      <th>family_SCHOOL AND OFFICE SUPPLIES</th>\n",
       "      <th>family_SEAFOOD</th>\n",
       "      <th>onpromotion_Promotion</th>\n",
       "      <th>holiday_type_Event</th>\n",
       "      <th>holiday_type_Holiday</th>\n",
       "      <th>holiday_type_No holiday</th>\n",
       "      <th>holiday_type_Transfer</th>\n",
       "      <th>transactions</th>\n",
       "      <th>oil_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.724057</td>\n",
       "      <td>0.95101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.724057</td>\n",
       "      <td>0.95101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.724057</td>\n",
       "      <td>0.95101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.724057</td>\n",
       "      <td>0.95101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.724057</td>\n",
       "      <td>0.95101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   family_BABY CARE  family_BEAUTY  family_BEVERAGES  family_BOOKS   \n",
       "0               0.0            0.0               0.0           0.0  \\\n",
       "1               1.0            0.0               0.0           0.0   \n",
       "2               0.0            1.0               0.0           0.0   \n",
       "3               0.0            0.0               1.0           0.0   \n",
       "4               0.0            0.0               0.0           1.0   \n",
       "\n",
       "   family_BREAD/BAKERY  family_CELEBRATION  family_CLEANING  family_DAIRY   \n",
       "0                  0.0                 0.0              0.0           0.0  \\\n",
       "1                  0.0                 0.0              0.0           0.0   \n",
       "2                  0.0                 0.0              0.0           0.0   \n",
       "3                  0.0                 0.0              0.0           0.0   \n",
       "4                  0.0                 0.0              0.0           0.0   \n",
       "\n",
       "   family_DELI  family_EGGS  ...  family_PRODUCE   \n",
       "0          0.0          0.0  ...             0.0  \\\n",
       "1          0.0          0.0  ...             0.0   \n",
       "2          0.0          0.0  ...             0.0   \n",
       "3          0.0          0.0  ...             0.0   \n",
       "4          0.0          0.0  ...             0.0   \n",
       "\n",
       "   family_SCHOOL AND OFFICE SUPPLIES  family_SEAFOOD  onpromotion_Promotion   \n",
       "0                                0.0             0.0                    0.0  \\\n",
       "1                                0.0             0.0                    0.0   \n",
       "2                                0.0             0.0                    0.0   \n",
       "3                                0.0             0.0                    0.0   \n",
       "4                                0.0             0.0                    0.0   \n",
       "\n",
       "   holiday_type_Event  holiday_type_Holiday  holiday_type_No holiday   \n",
       "0                 0.0                   1.0                      0.0  \\\n",
       "1                 0.0                   1.0                      0.0   \n",
       "2                 0.0                   1.0                      0.0   \n",
       "3                 0.0                   1.0                      0.0   \n",
       "4                 0.0                   1.0                      0.0   \n",
       "\n",
       "   holiday_type_Transfer  transactions  oil_price  \n",
       "0                    0.0     -1.724057    0.95101  \n",
       "1                    0.0     -1.724057    0.95101  \n",
       "2                    0.0     -1.724057    0.95101  \n",
       "3                    0.0     -1.724057    0.95101  \n",
       "4                    0.0     -1.724057    0.95101  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the encoded X_val DataFrame and the scaled X_val DataFrame to have the ready X_val DataFrame\n",
    "\n",
    "X_val = pd.concat([X_val_cat_encoded_df, X_val_num_scaled_df.set_axis(X_val_cat_encoded_df.index)], axis=1)\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd266dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = train_merged.loc[:, 'transactions'].values\n",
    "# series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255da1a",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The following models will be built and evaluated:\n",
    "1. Linear Regression\n",
    "\n",
    "2. XGBoost\n",
    "\n",
    "3. CatBoost\n",
    "\n",
    "4. ARIMA\n",
    "\n",
    "5. SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf203277",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def evaluate_time_series_models(data, date_column, target_column):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     # Assuming you have your time series data in a DataFrame 'data'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     # Replace 'date_column' with the column containing date/time and 'target_column' with the target variable column name\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize model names and corresponding models\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARIMA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSARIMA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m models \u001b[38;5;241m=\u001b[39m [LinearRegression(), xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(), CatBoostRegressor(), \u001b[43mARIMA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     15\u001b[0m           SARIMAX(train, order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), seasonal_order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m12\u001b[39m))]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to store the evaluation results\u001b[39;00m\n\u001b[0;32m     18\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSLE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSLE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\arima\\model.py:158\u001b[0m, in \u001b[0;36mARIMA.__init__\u001b[1;34m(self, endog, exog, order, seasonal_order, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[0;32m    151\u001b[0m     trend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Construct the specification\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# (don't pass specific values of enforce stationarity/invertibility,\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# because we don't actually want to restrict the estimators based on\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# this criteria. Instead, we'll just make sure that the parameter\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# estimates from those methods satisfy the criteria.)\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_arima \u001b[38;5;241m=\u001b[39m \u001b[43mSARIMAXSpecification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_stationarity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_invertibility\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcentrate_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcentrate_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrend_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_specification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_specification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m exog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_arima\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39morig_exog\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Raise an error if we have a constant in an integrated model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\arima\\specification.py:446\u001b[0m, in \u001b[0;36mSARIMAXSpecification.__init__\u001b[1;34m(self, endog, exog, order, seasonal_order, ar_order, diff, ma_order, seasonal_ar_order, seasonal_diff, seasonal_ma_order, seasonal_periods, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[0;32m    441\u001b[0m         exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[trend_data, exog]\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Create an underlying time series model, to handle endog / exog,\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# especially validating shapes, retrieving names, and potentially\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# providing us with a time series index\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m faux_endog \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mendog\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:470\u001b[0m, in \u001b[0;36mTimeSeriesModel.__init__\u001b[1;34m(self, endog, exog, dates, freq, missing, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    469\u001b[0m ):\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;66;03m# Date handling in indexes\u001b[39;00m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_dates(dates, freq)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\base\\data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[1;34m(self, endog, exog)\u001b[0m\n\u001b[0;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(PandasData, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[1;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "\n",
    "# def evaluate_time_series_models(data, date_column, target_column):\n",
    "#     # Assuming you have your time series data in a DataFrame 'data'\n",
    "#     # Replace 'date_column' with the column containing date/time and 'target_column' with the target variable column name\n",
    "#     data[date_column] = pd.to_datetime(data[date_column])\n",
    "#     data.set_index(date_column, inplace=True)\n",
    "#     target = data[target_column]\n",
    "\n",
    "#     # Splitting the data into training and testing sets (you can adjust the split ratio as per your requirement)\n",
    "#     train_size = int(len(target) * 0.8)\n",
    "#     train, test = target[:train_size], target[train_size:]\n",
    "\n",
    "# Initialize model names and corresponding models\n",
    "model_names = ['Linear Regression', 'XGBoost', 'CatBoost', 'ARIMA', 'SARIMA']\n",
    "models = [LinearRegression(), xgb.XGBRegressor(), CatBoostRegressor(), ARIMA(train, order=(1, 0, 0)),\n",
    "          SARIMAX(train, order=(1, 0, 0), seasonal_order=(1, 0, 0, 12))]\n",
    "\n",
    "# Create a DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "    if model_name in ['ARIMA', 'SARIMA']:\n",
    "        # Fit the ARIMA or SARIMA model\n",
    "        model_fit = model.fit()\n",
    "\n",
    "        # Make predictions\n",
    "        model_preds = model_fit.forecast(steps=len(test))\n",
    "    else:\n",
    "        # Fit other models\n",
    "        model.fit(np.arange(len(train)).reshape(-1, 1), train)\n",
    "\n",
    "        # Make predictions\n",
    "        model_preds = model.predict(np.arange(len(test)).reshape(-1, 1))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(test, model_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(test, model_preds))\n",
    "    msle = mean_squared_log_error(test, model_preds)\n",
    "\n",
    "    # Append the results to the DataFrame\n",
    "    results = results.append({'Model': model_name, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'MSLE': msle},\n",
    "                             ignore_index=True)\n",
    "\n",
    "return results\n",
    "\n",
    "# Assuming you have your time series data in a DataFrame 'data' with columns 'Date' and 'Target'\n",
    "# Adjust the column names accordingly\n",
    "results = evaluate_time_series_models(train_merged, date_column='Date', target_column='y_val')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "720fc8f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/target/data_providers.cpp:612: Currently only multi-regression, multilabel and survival objectives work with multidimensional target",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m catboost_model \u001b[38;5;241m=\u001b[39m CatBoostRegressor()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcatboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m catboost_preds \u001b[38;5;241m=\u001b[39m catboost_model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(X_val))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:5734\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5732\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5735\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5736\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5737\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:2357\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2353\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2356\u001b[0m     plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2365\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2366\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\catboost\\core.py:1761\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1761\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4624\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4673\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/private/libs/target/data_providers.cpp:612: Currently only multi-regression, multilabel and survival objectives work with multidimensional target"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "catboost_model = CatBoostRegressor()\n",
    "\n",
    "# Fit the model\n",
    "catboost_model.fit(np.arange(len(X_train)).reshape(-1, 1), X_train)\n",
    "\n",
    "# Make predictions\n",
    "catboost_preds = catboost_model.predict(np.arange(len(X_val)).reshape(-1, 1))\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse_catboost = mean_squared_error(X_val, catboost_preds)\n",
    "rmse_catboost = np.sqrt(mse_catboost)\n",
    "rmsle_catboost = np.sqrt(mean_squared_log_error(X_val, catboost_preds))\n",
    "msle_catboost = mean_squared_log_error(X_val, catboost_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d33545",
   "metadata": {},
   "source": [
    "### AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e20cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pacf and acf\n",
    "\n",
    "pacf = plot_pacf(data['sales'], lags=10)\n",
    "acf = plot_acf(data['sales'], lags=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoReg(data, lags=5).fit()\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_pred = model.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c43257",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(AR_pred, label='AR Forecast')\n",
    "plt.title('AutoRegressive (AR) Method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, AR_pred)\n",
    "msle = mean_squared_log_error(val, AR_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2)\n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "ARIMA_results = pd.DataFrame([['AR', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(ARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07e55b",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d025217",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_fit = autoarima(train['sales'], trace=True, suppress_warnings=True)\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50fda4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train, order=(1,1,1))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809315a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "\n",
    "ARIMA_pred = model.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "ARIMA_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(ARIMA_pred, label='ARIMA Forecast')\n",
    "plt.title('AutoRegressive Integrated Moving Average (ARIMA) method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, ARIMA_pred)\n",
    "msle = mean_squared_log_error(val, ARIMA_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2) \n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "ARIMA_results = pd.DataFrame([['ARIMA', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(ARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fcc57",
   "metadata": {},
   "source": [
    "### SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40272c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation set\n",
    "\n",
    "SARIMA_pred = model.predict(start=len(train), end=len(train)+len(test)-1, typ='levels')\n",
    "SARIMA_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['sales'], label='Train')\n",
    "plt.plot(val['sales'], label='Validation')\n",
    "plt.plot(SARIMA_pred, label='SARIMA Forecast')\n",
    "plt.title('Seasonal AutoRegressive Integrated Moving Average (SARIMA) method')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(val, SARIMA_pred)\n",
    "msle = mean_squared_log_error(val, SARIMA_pred)\n",
    "rmse = np.sqrt(mse).round(2)\n",
    "rmsle = np.sqrt(msle).round(2) \n",
    "\n",
    "# Create a new row for the evaluation metrics of ARIMA model on the results DataFrame\n",
    "SARIMA_results = pd.DataFrame([['SARIMA', 'mse', 'rmse', 'rmsle', 'msle']], columns=['Model', 'MSE', 'RMSE', 'RMSLE', 'MSLE'])\n",
    "results = results.append(SARIMA_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5321872",
   "metadata": {},
   "source": [
    "# Make series stationary\n",
    "\n",
    "We will use the boxcox method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32826081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox = pd.Series(boxcox(train['sales'], lbda=0), index=train.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox, label='Before Boxcox Transformation')\n",
    "plt.title('Before Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c35e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_boxcox_diff = pd.Series(data_boxcox - data_boxcox.shift(), index=train.index)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(data_boxcox_diff, label='After Boxcox Transformation')\n",
    "plt.title('After Boxcox Transformation')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af92ed9",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad0bde",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdc858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
